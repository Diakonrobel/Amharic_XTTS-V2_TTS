# ğŸš€ Checkpoint Selection - Quick Start

## âœ… What You Can Do NOW

Your WebUI now has a **Checkpoint Selection** feature that lets you:
- Browse all saved checkpoints from your training
- See which checkpoint has the best evaluation loss
- Automatically detect overfitting
- Select and use any checkpoint with one click

## ğŸ¯ Quick Usage (3 Steps)

### Step 1: Start WebUI
```bash
python xtts_demo.py
```

### Step 2: Go to Inference Tab â†’ Checkpoint Selection

Scroll down to the **"ğŸ”„ Checkpoint Selection (Advanced)"** section

### Step 3: Click Buttons

1. **ğŸ” Scan Checkpoints** - Lists all your checkpoints
2. **ğŸ“Š Analyze Overfitting** - Checks if training overfitted
3. Select a checkpoint from dropdown
4. **âœ… Use Selected Checkpoint** - Copies it to ready folder
5. **â–¶ï¸ Step 3 - Load Model** - Loads the selected checkpoint

Done! Now generate speech with the better checkpoint! ğŸ¤

## ğŸ’¡ For Your Amharic Case

Your training went for **91 epochs** and **severely overfitted**. Here's what to do:

```
1. Click "ğŸ” Scan Checkpoints"
   â†’ You'll see: best_model_569.pth (Epoch 0, Loss: 3.415) â† RECOMMENDED

2. Click "ğŸ“Š Analyze Overfitting"
   â†’ You'll see: "OVERFITTING DETECTED! Use early checkpoint"

3. Select "best_model_569.pth" from dropdown (should be pre-selected)

4. Click "âœ… Use Selected Checkpoint"
   â†’ Success message appears

5. Click "â–¶ï¸ Step 3 - Load Model"
   â†’ Model loads

6. Enter Amharic text and generate!
   â†’ MUCH BETTER quality than the final model!
```

## ğŸ“– Documentation

- **Full Guide**: `CHECKPOINT_SELECTION_GUIDE.md` (364 lines, comprehensive)
- **Implementation**: `CHECKPOINT_FEATURE_SUMMARY.md` (technical details)
- **Training Fix**: `TRAINING_DIAGNOSIS_AND_FIX.md` (overfitting solutions)

## ğŸ› ï¸ Helper Scripts

### Extract Eval Losses Manually

If you want to see your training's evaluation losses in detail:

```bash
python extract_checkpoint_losses.py
```

This will:
- Parse your training log
- Show all evaluation losses per epoch
- Detect overfitting automatically
- Save results to `checkpoint_eval_losses.txt`

## ğŸ‰ Benefits

âœ… **Immediate**: Use a better checkpoint right now (no retraining!)  
âœ… **Automatic**: System recommends the best checkpoint  
âœ… **Safe**: Automatic backups, never lose models  
âœ… **Easy**: 3 clicks to select and load  

## â“ FAQ

**Q: Where are my checkpoints?**  
A: `finetune_models/run/training/GPT_XTTS_FT-[DATE]/checkpoint_*.pth`

**Q: Will this delete my current model?**  
A: No! It creates a backup first: `model_backup_YYYYMMDD_HHMMSS.pth`

**Q: How do I know which checkpoint is best?**  
A: Click "ğŸ” Scan Checkpoints" - the system automatically recommends one with a ğŸ’¡ icon

**Q: What if I don't see any checkpoints?**  
A: You need to train a model first. Checkpoints are saved every 1000 steps during training.

**Q: Can I use an older training run?**  
A: The UI shows the latest run. For older runs, manually copy the checkpoint:
```bash
copy finetune_models\run\training\OLD_RUN\checkpoint_3000.pth finetune_models\ready\model.pth
```

## ğŸ› Troubleshooting

**Issue**: "No checkpoints found"  
**Fix**: Train a model first, or check `finetune_models/run/training/` exists

**Issue**: Scan button does nothing  
**Fix**: Check console for errors, ensure training completed

**Issue**: Selected checkpoint doesn't sound better  
**Fix**: Try a different checkpoint, or check dataset quality

## ğŸš€ Next Steps

1. âœ… Try the feature now (it's ready!)
2. âœ… Read `CHECKPOINT_SELECTION_GUIDE.md` for full details
3. âœ… Retrain with 10-15 epochs (not 100!) using lessons learned
4. âœ… Always scan checkpoints after training from now on!

---

**Feature Status**: âœ… **READY TO USE**  
**Your Action**: Start WebUI â†’ Go to Inference tab â†’ Try it!

Have questions? Check the console output - it's very detailed!
