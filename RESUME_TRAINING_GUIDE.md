# Resume Training vs Fresh Training Guide

## ğŸ”„ When to Resume Training

**Use Resume Training when:**
- âœ… You want to add more epochs to the same dataset
- âœ… Training was interrupted (power loss, crash, manual stop)
- âœ… Experimenting with different epoch counts
- âœ… **The dataset has NOT changed**

**How:**
1. Keep dataset unchanged
2. Enable "Resume from Checkpoint"
3. Select checkpoint from dropdown
4. Train continues seamlessly

---

## ğŸ†• When to Start Fresh Training (with Checkpoint as Base)

**Use Fresh Training when:**
- âœ… You added new audio files to dataset
- âœ… You modified existing transcripts
- âœ… You changed preprocessing settings (VAD, G2P, etc.)
- âœ… You want incremental learning with new data

**How:**
1. Process new/modified dataset normally
2. Load parameters from output folder
3. **Enter previous checkpoint in "Custom Model Path":**
   ```
   output/run/training/best_model.pth
   OR
   output/ready/model.pth (if you optimized it)
   ```
4. **Disable "Resume from Checkpoint"** (leave unchecked)
5. Start training - it will use the old model as base but with fresh optimizer

---

## Why This Matters

### Resume Training (Same Dataset)
```
Checkpoint â†’ [Model + Optimizer State + Dataset State]
          â†“
   Continue exactly where you left off
```

### Fresh Training with Base Model (New Dataset)
```
Old Model â†’ [Model Weights Only]
          â†“
   New Optimizer + New Dataset
          â†“
   Stable incremental learning
```

---

## Example Workflows

### Workflow 1: Add More Epochs (Same Dataset)
```bash
# Initial training
Epochs: 10
Result: output/run/training/best_model.pth

# Continue for 5 more epochs
â˜‘ï¸ Resume from Checkpoint
Checkpoint: run/training/best_model.pth
Epochs: 5 (additional)
Result: Continues from epoch 10 â†’ 15
```

### Workflow 2: Add New Dataset (Incremental Learning)
```bash
# Initial training
Dataset: 500 audio files
Epochs: 10
Result: output/ready/model.pth (after optimization)

# Add 300 more audio files
1. Process new dataset (total now 800 files)
2. Custom Model Path: output/ready/model.pth
3. â˜ Resume from Checkpoint (unchecked)
4. Epochs: 10 (fresh training with all 800 files)
Result: New model trained on 800 files, starting from previous knowledge
```

### Workflow 3: Fix Dataset Issues
```bash
# Initial training
Dataset: Had some bad audio
Epochs: 10
Result: output/run/training/checkpoint_3000.pth (early checkpoint)

# Fixed dataset (removed bad audio, fixed transcripts)
1. Process corrected dataset
2. Custom Model Path: output/run/training/checkpoint_3000.pth
3. â˜ Resume from Checkpoint (unchecked)
4. Epochs: 10 (fresh training with clean dataset)
Result: Better quality model
```

---

## Technical Details

### What's in a Checkpoint?

```python
checkpoint = {
    'model': model_state_dict,           # âœ… Model weights
    'optimizer': optimizer_state_dict,    # âš ï¸ Tied to dataset
    'epoch': current_epoch,               # âš ï¸ Tied to dataset size
    'iteration': current_iteration,       # âš ï¸ Tied to dataset size
    'scheduler': lr_scheduler_state,      # âš ï¸ Tied to training schedule
}
```

### Resume Training (Uses Everything)
- Loads all checkpoint state
- Expects exact same dataset
- Continues learning rate schedule
- Optimizer momentum matches old gradients

### Fresh Training with Base Model (Model Only)
- Uses only model weights
- Fresh optimizer state
- Fresh learning rate schedule
- Adapts to new dataset naturally

---

## Best Practices

1. **Always backup checkpoints before modifying dataset**
   ```bash
   cp -r output/run/training output/run/training_backup
   ```

2. **Keep training logs organized**
   - Use different output folders for different dataset versions
   - Name folders descriptively: `output_v1_500samples`, `output_v2_800samples`

3. **Test new data incrementally**
   - Don't add 1000 files at once
   - Add 100-200 at a time and retrain
   - Monitor quality after each addition

4. **Document your datasets**
   ```
   dataset_v1/
   â”œâ”€â”€ metadata_train.csv  (500 samples)
   â””â”€â”€ lang.txt
   
   dataset_v2/
   â”œâ”€â”€ metadata_train.csv  (800 samples, +300 new)
   â””â”€â”€ lang.txt
   â””â”€â”€ CHANGES.txt  (what was added/fixed)
   ```

---

## Quick Decision Tree

```
Do you want to continue training?
â”‚
â”œâ”€ Same dataset? â†’ âœ… Use Resume Training
â”‚  â””â”€ Enable "Resume from Checkpoint"
â”‚
â””â”€ Different dataset? â†’ âœ… Use Fresh Training with Base Model
   â””â”€ Custom Model Path: previous checkpoint
   â””â”€ Disable "Resume from Checkpoint"
```

---

## Summary

| Scenario | Resume Training | Custom Model Path | Result |
|----------|----------------|-------------------|---------|
| Add more epochs (same data) | âœ… Yes | Leave empty | Continue smoothly |
| Add new audio files | âŒ No | Previous model | Stable incremental learning |
| Fix dataset errors | âŒ No | Early checkpoint | Fresh start with fixed data |
| Interrupted training | âœ… Yes | Leave empty | Recover training |
| Experiment with epochs | âœ… Yes | Leave empty | Try different lengths |

**Golden Rule:** 
- **Resume = Same Dataset Only**
- **Dataset Changed = Fresh Training + Custom Model**
