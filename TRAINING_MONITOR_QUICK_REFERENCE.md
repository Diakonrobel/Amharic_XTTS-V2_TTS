# ğŸ¯ Training Monitoring Quick Reference

**Keep this open while training to make quick decisions!**

---

## âœ… Decision Matrix

| Epoch | Train Loss | Eval Loss | Gap | Action |
|-------|-----------|-----------|-----|--------|
| 0 | 0.45 | 0.48 | 6.7% | âœ… **Continue** - Healthy start |
| 1 | 0.35 | 0.40 | 14% | âœ… **Continue** - Good progress |
| 2 | 0.28 | 0.36 | 29% | âœ… **Continue** - Acceptable gap |
| 3 | 0.24 | 0.34 | 42% | âš ï¸ **Monitor** - Gap widening |
| 4 | 0.21 | 0.40 | 90% | ğŸ›‘ **STOP** - Eval increased! |

---

## ğŸš¦ Traffic Light System

### âœ… GREEN - Continue Training
```
Train Loss: Decreasing â†“
Eval Loss:  Decreasing â†“
Gap:        <30%

Example:
  Epoch 2: train=0.32, eval=0.38 (19% gap) âœ…
```

### âš ï¸ YELLOW - Watch Carefully
```
Train Loss: Decreasing â†“
Eval Loss:  Flat or small increase â†’
Gap:        30-50%

Example:
  Epoch 3: train=0.25, eval=0.38 (52% gap) âš ï¸
  â†’ Watch next epoch closely
```

### ğŸ›‘ RED - STOP IMMEDIATELY
```
Train Loss: Decreasing â†“
Eval Loss:  Increasing â†‘ (2 epochs in a row)
Gap:        >100%

Example:
  Epoch 3: train=0.24, eval=0.34 (OK)
  Epoch 4: train=0.20, eval=0.40 (BAD - increased!)
  Epoch 5: train=0.18, eval=0.50 (WORSE - STOP!) ğŸ›‘
  
  â†’ Use checkpoint from Epoch 3
```

---

## ğŸ“Š Epoch-by-Epoch Checklist

### After Each Epoch:

1. **Find the EVALUATION section** in logs
   ```
   > EVALUATION
   > avg_loss: X.XXX
   ```

2. **Record the losses:**
   - Training loss (last step before eval)
   - Eval loss (from EVALUATION section)

3. **Calculate gap:**
   ```
   Gap % = ((eval_loss - train_loss) / train_loss) Ã— 100
   ```

4. **Make decision:**
   - Gap <30% â†’ âœ… Continue
   - Gap 30-50% â†’ âš ï¸ Watch next epoch
   - Gap >50% OR eval increasing 2x â†’ ğŸ›‘ STOP

---

## ğŸ¯ Quick Commands

### Monitor Training (Real-time)
```bash
# On Lightning AI
tail -f finetune_models/run/training/GPT_XTTS_FT-*/trainer_0_log.txt | grep -E "EPOCH|EVALUATION|avg_loss"
```

### Find Eval Losses
```bash
grep "EVALUATION" finetune_models/run/training/GPT_XTTS_FT-*/trainer_0_log.txt -A 5
```

### Stop Training
```
Ctrl+C in terminal
OR
Kill process in Lightning AI
```

---

## ğŸ“‹ Training Log Template

**Fill this out after each epoch:**

```
=== TRAINING PROGRESS ===

Epoch 0:
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

Epoch 1:
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

Epoch 2: [LR reduces to 1e-06]
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

Epoch 3:
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

Epoch 4: [LR reduces to 5e-07]
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

Epoch 5:
  Train Loss: _____
  Eval Loss:  _____
  Gap:        _____% 
  Decision:   âœ… Continue / âš ï¸ Watch / ğŸ›‘ Stop

BEST EPOCH: _____ (lowest eval_loss)
```

---

## ğŸ¯ What to Look For

### Good Signs âœ…
- Both losses decreasing steadily
- Gap stays below 30%
- LR reductions cause slight slowdown (expected)
- Eval loss keeps improving or stable

### Warning Signs âš ï¸
- Gap increasing beyond 30%
- Eval loss stops improving
- Training loss approaches zero too fast
- Large fluctuations in eval loss

### Stop Signs ğŸ›‘
- Eval loss increases 2 epochs in a row
- Gap exceeds 100%
- Training loss near zero but eval loss high
- Audio quality gets worse

---

## ğŸš€ After Training Stops

### 1. Identify Best Checkpoint
```
Best epoch = epoch with LOWEST eval_loss
```

### 2. Use Checkpoint Manager
```
1. Open WebUI â†’ Inference tab
2. Click "Scan Training Runs"
3. Find run "GPT_XTTS_FT-October-14-2025..."
4. Select checkpoint from best epoch
5. Click "Copy to Ready Folder"
6. Test audio generation
```

### 3. Test Quality
Generate sample audio with Amharic text and verify:
- âœ… Clear pronunciation
- âœ… Natural prosody
- âœ… No artifacts
- âœ… Correct Amharic phonemes

---

## ğŸ’¡ Pro Tips

1. **Check eval loss IMMEDIATELY** after each epoch finishes
2. **Don't wait** - if you see 2 consecutive increases, stop NOW
3. **Keep this doc open** on second monitor/tab
4. **Set alarm** for end of each epoch (~10-15 min)
5. **Screenshot eval losses** for record-keeping

---

## ğŸ”¢ Example Calculations

### Scenario 1: Good Training
```
Epoch 2:
  train_loss = 0.30
  eval_loss = 0.36
  
Gap = ((0.36 - 0.30) / 0.30) Ã— 100 = 20%
Decision: âœ… Continue (gap <30%)
```

### Scenario 2: Warning
```
Epoch 3:
  train_loss = 0.25
  eval_loss = 0.38
  
Gap = ((0.38 - 0.25) / 0.25) Ã— 100 = 52%
Decision: âš ï¸ Watch closely (gap >50%)
```

### Scenario 3: Stop
```
Epoch 3: eval_loss = 0.34 âœ…
Epoch 4: eval_loss = 0.40 âš ï¸ (increased by 0.06)
Epoch 5: eval_loss = 0.48 ğŸ›‘ (increased again by 0.08)

Decision: ğŸ›‘ STOP NOW - use checkpoint from Epoch 3
```

---

## ğŸ“ Need Help?

**If eval_loss is confusing:**
- Lower = Better
- Should decrease over epochs
- Closer to train_loss = Better generalization

**If gap is confusing:**
```
Gap = How much worse model performs on unseen data
<20% = Excellent
20-30% = Good
30-50% = Concerning
>50% = Overfitting
```

---

**Keep this reference handy during training! Good luck!** ğŸš€
