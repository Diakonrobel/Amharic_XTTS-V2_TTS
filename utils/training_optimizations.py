"""
Training Optimizations for XTTS Fine-tuning

This module provides memory and speed optimizations for XTTS training:
1. Gradient Checkpointing - 20-30% memory reduction
2. PyTorch SDPA (Scaled Dot Product Attention) - 1.3-1.5x speed, 30-40% memory reduction
3. Mixed Precision Training - Additional speedup with compatible GPUs
4. Memory-efficient attention backends

Based on best practices from:
- PyTorch 2.0+ SDPA documentation
- Unsloth TTS optimization techniques
- HuggingFace Trainer optimizations
"""

import torch
import logging
from typing import Optional, Dict, Any
import warnings

logger = logging.getLogger(__name__)


class TrainingOptimizer:
    """
    Manages training optimizations for XTTS fine-tuning
    
    Features:
    - Gradient checkpointing
    - SDPA (Scaled Dot Product Attention)
    - Mixed precision training
    - Memory profiling
    - Automatic fallback on errors
    """
    
    def __init__(
        self,
        enable_gradient_checkpointing: bool = False,
        enable_sdpa: bool = False,
        enable_mixed_precision: bool = False,
        verbose: bool = True
    ):
        """
        Initialize training optimizer
        
        Args:
            enable_gradient_checkpointing: Enable gradient checkpointing for memory savings
            enable_sdpa: Enable PyTorch SDPA for faster attention
            enable_mixed_precision: Enable mixed precision training (fp16/bf16)
            verbose: Print optimization status messages
        """
        self.enable_gradient_checkpointing = enable_gradient_checkpointing
        self.enable_sdpa = enable_sdpa
        self.enable_mixed_precision = enable_mixed_precision
        self.verbose = verbose
        
        self.cuda_available = torch.cuda.is_available()
        self.pytorch_version = tuple(map(int, torch.__version__.split('.')[:2]))
        
        # Check compatibility
        self._check_compatibility()
    
    def _check_compatibility(self):
        """Check hardware and software compatibility"""
        if not self.cuda_available:
            if self.verbose:
                logger.warning("‚ö†Ô∏è  CUDA not available - optimizations will be limited")
            self.enable_sdpa = False
            self.enable_mixed_precision = False
            return
        
        # Check PyTorch version for SDPA
        if self.enable_sdpa and self.pytorch_version < (2, 0):
            if self.verbose:
                logger.warning(f"‚ö†Ô∏è  SDPA requires PyTorch 2.0+, current: {torch.__version__}")
                logger.info("   Disabling SDPA optimization")
            self.enable_sdpa = False
        
        # Check GPU compute capability for optimal features
        if self.cuda_available:
            compute_capability = torch.cuda.get_device_capability()
            if compute_capability[0] < 7:
                if self.verbose:
                    logger.warning(f"‚ö†Ô∏è  GPU compute capability {compute_capability} < 7.0")
                    logger.info("   Some optimizations may not be available")
    
    def configure_sdpa(self) -> bool:
        """
        Configure PyTorch Scaled Dot Product Attention
        
        Returns:
            True if SDPA was successfully enabled
        """
        if not self.enable_sdpa or not self.cuda_available:
            return False
        
        try:
            # Enable memory-efficient attention backends
            # PyTorch will automatically select the best backend:
            # 1. Flash Attention 2 (if available on hardware)
            # 2. Memory-efficient attention (xformers-style)
            # 3. Math attention (fallback)
            
            if self.pytorch_version >= (2, 0):
                # Enable all SDPA backends for automatic selection
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                torch.backends.cuda.enable_math_sdp(True)  # Fallback
                
                if self.verbose:
                    logger.info("‚úÖ SDPA enabled with automatic backend selection")
                    logger.info("   Will use: Flash Attention > Memory-Efficient > Math")
                
                return True
            else:
                if self.verbose:
                    logger.warning("‚ö†Ô∏è  PyTorch version too old for SDPA")
                return False
                
        except Exception as e:
            if self.verbose:
                logger.error(f"‚ùå Failed to enable SDPA: {e}")
            return False
    
    def apply_gradient_checkpointing(self, model) -> bool:
        """
        Apply gradient checkpointing to model
        
        Args:
            model: The XTTS model to optimize
            
        Returns:
            True if gradient checkpointing was successfully applied
        """
        if not self.enable_gradient_checkpointing:
            return False
        
        try:
            # Apply gradient checkpointing to transformer layers
            if hasattr(model, 'xtts') and hasattr(model.xtts, 'gpt'):
                gpt_model = model.xtts.gpt
                
                # Enable gradient checkpointing on GPT layers
                if hasattr(gpt_model, 'gradient_checkpointing_enable'):
                    gpt_model.gradient_checkpointing_enable()
                    if self.verbose:
                        logger.info("‚úÖ Gradient checkpointing enabled on GPT layers")
                    return True
                
                # Manual gradient checkpointing for transformer blocks
                elif hasattr(gpt_model, 'layers') or hasattr(gpt_model, 'h'):
                    layers = gpt_model.layers if hasattr(gpt_model, 'layers') else gpt_model.h
                    
                    for layer in layers:
                        if hasattr(layer, 'gradient_checkpointing'):
                            layer.gradient_checkpointing = True
                    
                    if self.verbose:
                        logger.info(f"‚úÖ Gradient checkpointing enabled on {len(layers)} transformer layers")
                    return True
                else:
                    if self.verbose:
                        logger.warning("‚ö†Ô∏è  Model doesn't support gradient checkpointing")
                    return False
            else:
                if self.verbose:
                    logger.warning("‚ö†Ô∏è  Model structure not compatible with gradient checkpointing")
                return False
                
        except Exception as e:
            if self.verbose:
                logger.error(f"‚ùå Failed to apply gradient checkpointing: {e}")
            return False
    
    def get_mixed_precision_config(self) -> Dict[str, Any]:
        """
        Get mixed precision training configuration
        
        Returns:
            Configuration dict for mixed precision training
        """
        if not self.enable_mixed_precision or not self.cuda_available:
            return {}
        
        config = {}
        
        try:
            # Check if bfloat16 is supported (Ampere+ GPUs)
            if torch.cuda.is_bf16_supported():
                config['precision'] = 'bf16'
                if self.verbose:
                    logger.info("‚úÖ Mixed precision: bfloat16 (Ampere+ GPU detected)")
            else:
                config['precision'] = 'fp16'
                if self.verbose:
                    logger.info("‚úÖ Mixed precision: float16")
            
            config['use_amp'] = True
            
        except Exception as e:
            if self.verbose:
                logger.warning(f"‚ö†Ô∏è  Mixed precision not available: {e}")
        
        return config
    
    def optimize_model(self, model):
        """
        Apply all enabled optimizations to the model
        
        Args:
            model: The XTTS model to optimize
            
        Returns:
            Optimization status dict
        """
        status = {
            'gradient_checkpointing': False,
            'sdpa': False,
            'mixed_precision': {}
        }
        
        if self.verbose:
            logger.info("\n" + "="*70)
            logger.info("üöÄ Applying Training Optimizations")
            logger.info("="*70)
        
        # Apply gradient checkpointing
        if self.enable_gradient_checkpointing:
            status['gradient_checkpointing'] = self.apply_gradient_checkpointing(model)
        
        # Configure SDPA
        if self.enable_sdpa:
            status['sdpa'] = self.configure_sdpa()
        
        # Get mixed precision config
        if self.enable_mixed_precision:
            status['mixed_precision'] = self.get_mixed_precision_config()
        
        # Print summary
        if self.verbose:
            self._print_optimization_summary(status)
        
        return status
    
    def _print_optimization_summary(self, status: Dict[str, Any]):
        """Print optimization summary"""
        logger.info("\nüìä Optimization Summary:")
        logger.info("-" * 70)
        
        if status['gradient_checkpointing']:
            logger.info("‚úÖ Gradient Checkpointing: ENABLED")
            logger.info("   ‚îî‚îÄ Expected: 20-30% memory reduction")
        else:
            logger.info("‚ùå Gradient Checkpointing: DISABLED")
        
        if status['sdpa']:
            logger.info("‚úÖ SDPA (Fast Attention): ENABLED")
            logger.info("   ‚îî‚îÄ Expected: 1.3-1.5x speed, 30-40% memory reduction")
        else:
            logger.info("‚ùå SDPA (Fast Attention): DISABLED")
        
        if status['mixed_precision']:
            precision = status['mixed_precision'].get('precision', 'None')
            logger.info(f"‚úÖ Mixed Precision: ENABLED ({precision})")
            logger.info("   ‚îî‚îÄ Expected: Additional speedup with compatible GPUs")
        else:
            logger.info("‚ùå Mixed Precision: DISABLED")
        
        logger.info("-" * 70)
        
        # Calculate total expected benefit
        total_speedup = 1.0
        total_memory_reduction = 0.0
        
        if status['gradient_checkpointing']:
            total_memory_reduction += 25  # 20-30% average
        
        if status['sdpa']:
            total_speedup *= 1.4  # 1.3-1.5x average
            total_memory_reduction += 35  # 30-40% average
        
        if status['mixed_precision']:
            total_speedup *= 1.2  # Additional 20% with AMP
        
        if total_speedup > 1.0 or total_memory_reduction > 0:
            logger.info("\nüéØ Expected Performance Improvements:")
            if total_speedup > 1.0:
                logger.info(f"   ‚ö° Training Speed: ~{total_speedup:.1f}x faster")
            if total_memory_reduction > 0:
                logger.info(f"   üíæ Memory Usage: ~{total_memory_reduction:.0f}% reduction")
        
        logger.info("="*70 + "\n")
    
    @staticmethod
    def get_memory_stats() -> Dict[str, float]:
        """
        Get current GPU memory statistics
        
        Returns:
            Dict with memory stats in GB
        """
        if not torch.cuda.is_available():
            return {}
        
        return {
            'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
            'reserved_gb': torch.cuda.memory_reserved() / 1024**3,
            'max_allocated_gb': torch.cuda.max_memory_allocated() / 1024**3,
        }
    
    @staticmethod
    def print_memory_stats():
        """Print current GPU memory statistics"""
        stats = TrainingOptimizer.get_memory_stats()
        if stats:
            logger.info("\nüíæ GPU Memory Usage:")
            logger.info(f"   Allocated: {stats['allocated_gb']:.2f} GB")
            logger.info(f"   Reserved:  {stats['reserved_gb']:.2f} GB")
            logger.info(f"   Peak:      {stats['max_allocated_gb']:.2f} GB\n")


def create_optimized_trainer_config(
    base_config,
    enable_gradient_checkpointing: bool = False,
    enable_sdpa: bool = False,
    enable_mixed_precision: bool = False
):
    """
    Create optimized trainer configuration
    
    Args:
        base_config: Base GPTTrainerConfig
        enable_gradient_checkpointing: Enable gradient checkpointing
        enable_sdpa: Enable SDPA
        enable_mixed_precision: Enable mixed precision
        
    Returns:
        Updated config with optimizations
    """
    # Note: Some optimizations are applied directly to the model,
    # others need to be configured in the trainer
    
    # Add optimization flags to config
    if hasattr(base_config, 'use_grad_checkpoint'):
        base_config.use_grad_checkpoint = enable_gradient_checkpointing
    
    return base_config


# Unsloth-inspired optimization utilities
class UnslothStyleOptimizations:
    """
    Additional optimizations inspired by Unsloth's TTS approach
    
    These are complementary techniques that can further improve
    training efficiency without modifying core model architecture
    """
    
    @staticmethod
    def optimize_dataloader(dataloader_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Optimize dataloader settings for better throughput
        
        Args:
            dataloader_config: Current dataloader configuration
            
        Returns:
            Optimized dataloader configuration
        """
        optimized = dataloader_config.copy()
        
        # Enable pin_memory for faster CPU->GPU transfers
        if torch.cuda.is_available():
            optimized['pin_memory'] = True
        
        # Use persistent workers to avoid recreation overhead
        if optimized.get('num_workers', 0) > 0:
            optimized['persistent_workers'] = True
        
        # Prefetch factor for better pipeline
        if optimized.get('num_workers', 0) > 0:
            optimized['prefetch_factor'] = 2
        
        return optimized
    
    @staticmethod
    def enable_cudnn_optimizations():
        """Enable cuDNN optimizations for consistent input sizes"""
        if torch.cuda.is_available():
            # Enable cuDNN autotuner
            torch.backends.cudnn.benchmark = True
            # Allow TF32 on Ampere+ GPUs for faster matmul
            if hasattr(torch.backends.cuda, 'matmul'):
                torch.backends.cuda.matmul.allow_tf32 = True
            if hasattr(torch.backends.cudnn, 'allow_tf32'):
                torch.backends.cudnn.allow_tf32 = True
            
            logger.info("‚úÖ cuDNN optimizations enabled")
    
    @staticmethod
    def compile_model(model, backend: str = "inductor"):
        """
        Compile model with torch.compile (PyTorch 2.0+)
        
        Args:
            model: Model to compile
            backend: Compilation backend ('inductor', 'aot_eager', etc.)
            
        Returns:
            Compiled model or original if compilation fails
        """
        if not hasattr(torch, 'compile'):
            logger.warning("‚ö†Ô∏è  torch.compile not available (requires PyTorch 2.0+)")
            return model
        
        try:
            logger.info(f"üîß Compiling model with {backend} backend...")
            compiled_model = torch.compile(model, backend=backend, mode="reduce-overhead")
            logger.info("‚úÖ Model compiled successfully")
            return compiled_model
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Model compilation failed: {e}")
            logger.info("   Continuing with uncompiled model")
            return model
