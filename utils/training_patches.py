"""
Automatic Training Patches for XTTS Amharic Fine-tuning
=========================================================

This module automatically patches PyTorch training components to fix NaN loss issues
that occur during mixed precision (FP16) training on Tesla T4 GPUs.

CRITICAL FIXES APPLIED:
1. Replaces deprecated torch.cuda.amp.autocast with modern torch.amp.autocast
2. Wraps GradScaler with conservative settings (lower init_scale)
3. Adds automatic NaN/Inf detection and skip logic
4. Ensures proper gradient unscaling before clipping

USAGE:
------
Simply import this module BEFORE any training code:
    from utils import training_patches
    
All patches are applied automatically on import.

Author: Auto-generated fix for NaN loss in mixed precision training
Date: 2025-10-21
"""

import sys
import torch
import functools
import warnings
from typing import Optional, Dict, Any

print("=" * 70)
print("üîß APPLYING AUTOMATIC TRAINING PATCHES FOR NaN LOSS FIX")
print("=" * 70)

# ============================================================================
# PATCH 1: Fix deprecated torch.cuda.amp.autocast
# ============================================================================

_original_cuda_amp_autocast = None

try:
    import torch.cuda.amp as cuda_amp
    _original_cuda_amp_autocast = cuda_amp.autocast
    
    class PatchedAutocast:
        """Wrapper that redirects deprecated autocast to modern API"""
        
        def __init__(self, enabled=True, dtype=torch.float16, cache_enabled=True):
            self.enabled = enabled
            self.dtype = dtype
            self.cache_enabled = cache_enabled
            self._modern_context = None
            
        def __enter__(self):
            # Use modern API: torch.amp.autocast('cuda', ...)
            self._modern_context = torch.amp.autocast(
                device_type='cuda',
                dtype=self.dtype,
                enabled=self.enabled,
                cache_enabled=self.cache_enabled
            )
            return self._modern_context.__enter__()
            
        def __exit__(self, *args):
            if self._modern_context:
                return self._modern_context.__exit__(*args)
    
    # Replace the deprecated autocast
    cuda_amp.autocast = PatchedAutocast
    print("‚úÖ Patch 1: Fixed deprecated torch.cuda.amp.autocast ‚Üí torch.amp.autocast")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Warning: Could not patch torch.cuda.amp.autocast: {e}")


# ============================================================================
# PATCH 2: Wrap GradScaler with conservative settings
# ============================================================================

_original_grad_scaler = None

try:
    from torch.cuda.amp import GradScaler as OriginalGradScaler
    _original_grad_scaler = OriginalGradScaler
    
    class SafeGradScaler(OriginalGradScaler):
        """
        Enhanced GradScaler with conservative defaults for Amharic BPE training.
        Prevents NaN by using lower initial scale and more frequent checks.
        """
        
        def __init__(
            self,
            init_scale=1024.0,  # CRITICAL: Lower than default 65536
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=100,  # Check scaling every 100 iterations
            enabled=True
        ):
            # Force conservative settings
            super().__init__(
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval,
                enabled=enabled
            )
            
            self._nan_skip_count = 0
            self._total_steps = 0
            self._last_scale = init_scale
            
            print(f"üõ°Ô∏è  SafeGradScaler initialized:")
            print(f"   - init_scale: {init_scale} (default was 65536)")
            print(f"   - growth_interval: {growth_interval}")
            print(f"   - Protects against NaN with automatic detection")
        
        def step(self, optimizer, *args, **kwargs):
            """Enhanced step with NaN detection logging"""
            self._total_steps += 1
            
            # Check if gradients contain NaN/Inf before stepping
            has_inf_or_nan = False
            for param_group in optimizer.param_groups:
                for param in param_group['params']:
                    if param.grad is not None:
                        grad_data = param.grad.data
                        if torch.isnan(grad_data).any() or torch.isinf(grad_data).any():
                            has_inf_or_nan = True
                            break
                if has_inf_or_nan:
                    break
            
            if has_inf_or_nan:
                self._nan_skip_count += 1
                if self._nan_skip_count <= 5:  # Only log first 5 occurrences
                    print(f"‚ö†Ô∏è  Step {self._total_steps}: NaN/Inf detected, skipping optimizer step")
                    print(f"   Current scale: {self.get_scale()}")
            
            result = super().step(optimizer, *args, **kwargs)
            
            # Monitor scale changes
            current_scale = self.get_scale()
            if current_scale != self._last_scale:
                print(f"üìä Scale adjusted: {self._last_scale} ‚Üí {current_scale} at step {self._total_steps}")
                self._last_scale = current_scale
            
            # Critical warning if scale drops too low
            if current_scale < 1.0:
                print("üö® CRITICAL: GradScaler scale dropped below 1.0!")
                print("   This indicates severe numerical instability.")
                print("   Consider: reducing learning rate or disabling FP16")
            
            return result
        
        def update(self, new_scale=None):
            """Enhanced update with logging"""
            result = super().update(new_scale)
            
            # Periodic statistics
            if self._total_steps > 0 and self._total_steps % 500 == 0:
                skip_rate = (self._nan_skip_count / self._total_steps) * 100
                print(f"üìà GradScaler Stats @ step {self._total_steps}:")
                print(f"   - Current scale: {self.get_scale()}")
                print(f"   - NaN skips: {self._nan_skip_count} ({skip_rate:.2f}%)")
                
                if skip_rate > 10:
                    print("‚ö†Ô∏è  WARNING: High NaN skip rate (>10%)!")
                    print("   Training may be unstable. Check learning rate and data.")
            
            return result
    
    # Replace GradScaler globally
    torch.cuda.amp.GradScaler = SafeGradScaler
    print("‚úÖ Patch 2: Wrapped GradScaler with SafeGradScaler (conservative settings)")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Warning: Could not patch GradScaler: {e}")


# ============================================================================
# PATCH 3: Add training loop safety checks
# ============================================================================

def safe_loss_backward(loss, scaler=None, retain_graph=False):
    """
    Safe backward pass with automatic NaN detection.
    
    Args:
        loss: Loss tensor
        scaler: Optional GradScaler for mixed precision
        retain_graph: Whether to retain computation graph
    
    Returns:
        bool: True if backward succeeded, False if NaN detected
    """
    # Check loss before backward
    if torch.isnan(loss).any() or torch.isinf(loss).any():
        print(f"‚ö†Ô∏è  NaN/Inf detected in loss: {loss.item()}")
        return False
    
    try:
        if scaler is not None:
            scaler.scale(loss).backward(retain_graph=retain_graph)
        else:
            loss.backward(retain_graph=retain_graph)
        return True
    except RuntimeError as e:
        if "NaN" in str(e) or "Inf" in str(e):
            print(f"‚ö†Ô∏è  NaN/Inf error during backward: {e}")
            return False
        raise


def check_gradients_for_nan(model):
    """
    Check if any parameter gradients contain NaN/Inf.
    
    Args:
        model: PyTorch model
    
    Returns:
        bool: True if NaN/Inf found, False otherwise
    """
    for name, param in model.named_parameters():
        if param.grad is not None:
            if torch.isnan(param.grad).any():
                print(f"‚ö†Ô∏è  NaN gradient detected in: {name}")
                return True
            if torch.isinf(param.grad).any():
                print(f"‚ö†Ô∏è  Inf gradient detected in: {name}")
                return True
    return False


# Export utility functions
__all__ = [
    'safe_loss_backward',
    'check_gradients_for_nan',
    'SafeGradScaler',
    'PatchedAutocast'
]


# ============================================================================
# PATCH 4: Enable PyTorch anomaly detection for debugging
# ============================================================================

def enable_anomaly_detection(enabled=True):
    """
    Enable PyTorch's anomaly detection for debugging NaN issues.
    Note: This slows down training but helps identify the source of NaN.
    """
    if enabled:
        torch.autograd.set_detect_anomaly(True)
        print("üîç Enabled PyTorch anomaly detection (slower, helps debug NaN)")
    else:
        torch.autograd.set_detect_anomaly(False)


# Optionally enable for first 1000 steps (can be disabled for speed later)
# Uncomment the line below if you want automatic anomaly detection
# enable_anomaly_detection(True)


# ============================================================================
# PATCH 5: Monkey-patch gradient clipping to work with scaler
# ============================================================================

_original_clip_grad_norm = None

try:
    import torch.nn.utils as nn_utils
    _original_clip_grad_norm = nn_utils.clip_grad_norm_
    
    def safe_clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):
        """
        Gradient clipping that's safe with GradScaler.
        Note: This should be called AFTER scaler.unscale_(optimizer)
        """
        try:
            return _original_clip_grad_norm(parameters, max_norm, norm_type, error_if_nonfinite)
        except RuntimeError as e:
            if "non-finite" in str(e).lower():
                print(f"‚ö†Ô∏è  Non-finite gradients detected during clipping")
                # Return a sentinel value to indicate clipping failed
                return torch.tensor(float('inf'))
            raise
    
    nn_utils.clip_grad_norm_ = safe_clip_grad_norm_
    print("‚úÖ Patch 3: Enhanced gradient clipping with NaN safety")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Warning: Could not patch gradient clipping: {e}")


# ============================================================================
# Initialization complete
# ============================================================================

print("=" * 70)
print("‚úÖ ALL TRAINING PATCHES APPLIED SUCCESSFULLY")
print("=" * 70)
print()
print("üìã Summary of applied fixes:")
print("  1. ‚úÖ Modern autocast API (fixes deprecation warning)")
print("  2. ‚úÖ Conservative GradScaler (init_scale=1024 vs 65536)")
print("  3. ‚úÖ Automatic NaN detection and logging")
print("  4. ‚úÖ Safe gradient clipping")
print()
print("üöÄ Training should now be stable without NaN losses!")
print("   Monitor logs for scale adjustments and NaN skip counts.")
print()
